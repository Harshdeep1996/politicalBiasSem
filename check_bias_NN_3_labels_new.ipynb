{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OnMCYi4yQiP_"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import glob\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from itertools import chain\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import FastText\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Bidirectional\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initializing tqdm for pandas\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "LaW4zx7hQiQL",
    "outputId": "36c112ce-14ac-44d5-a69b-97145f4b1192"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tensorflow'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CDWJs_1yQiQT",
    "outputId": "b65482ee-42b5-410a-9b8e-e3eb00e6f96f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "local_device_protos = device_lib.list_local_devices()\n",
    "print([x.name for x in local_device_protos if x.device_type == 'GPU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "y4o-UEKzQiQa"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GVqCO5LmQiQh",
    "outputId": "429fe9b0-080f-46a8-bc24-e3dd23b7a71e"
   },
   "outputs": [],
   "source": [
    "## Only for Google colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9qpl7i9QiQv"
   },
   "source": [
    "## Get auxiliary features and divide them into labels\n",
    "\n",
    "1. `ref_index`\n",
    "2. `total_words`\n",
    "3. `tags`\n",
    "4. `type_of_citation`\n",
    "\n",
    "#### can include `section` of the page in which the citation belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RFH9E72cQiQw"
   },
   "outputs": [],
   "source": [
    "citations_bias_features = pd.read_parquet('./mini_citations_bias_features.parquet/', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7M8dYUheQiQ2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LIBR    224058\n",
       "MODR    223476\n",
       "CONS    219993\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citations_bias_features['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1tmPtR2BQiRB"
   },
   "outputs": [],
   "source": [
    "cons_citations = citations_bias_features[citations_bias_features['label'] == 'CONS']\n",
    "modr_citations = citations_bias_features[citations_bias_features['label'] == 'MODR']\n",
    "libr_citations = citations_bias_features[citations_bias_features['label'] == 'LIBR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZSzoFnBhQiRH"
   },
   "outputs": [],
   "source": [
    "cons_citations = cons_citations.sample(n=210000)\n",
    "modr_citations = modr_citations.sample(n=210000)\n",
    "libr_citations = libr_citations.sample(n=210000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GsU-nypfQiRO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(630000, 18)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_with_features = pd.concat([cons_citations, modr_citations, libr_citations])\n",
    "dataset_with_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "aF8dimwpQiRY"
   },
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(dataset_with_features['label'])\n",
    "dataset_with_features['label_category'] = le.transform(dataset_with_features['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ZG4z2OGPQiRf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>tld</th>\n",
       "      <th>citations</th>\n",
       "      <th>Title</th>\n",
       "      <th>sections</th>\n",
       "      <th>type_of_citation</th>\n",
       "      <th>ID_list</th>\n",
       "      <th>id</th>\n",
       "      <th>r_id</th>\n",
       "      <th>r_parentid</th>\n",
       "      <th>page_title</th>\n",
       "      <th>page_id</th>\n",
       "      <th>ref_index</th>\n",
       "      <th>total_words</th>\n",
       "      <th>neighboring_words</th>\n",
       "      <th>neighboring_tags</th>\n",
       "      <th>bias_score</th>\n",
       "      <th>label</th>\n",
       "      <th>label_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>167129</th>\n",
       "      <td>http://www.katv.com/story/22467593/update-scot...</td>\n",
       "      <td>katv</td>\n",
       "      <td>{{cite web|title=Scott County Sheriff drowns d...</td>\n",
       "      <td>Scott County Sheriff drowns during rescue, 3 o...</td>\n",
       "      <td>Initial Section</td>\n",
       "      <td>cite web</td>\n",
       "      <td>None</td>\n",
       "      <td>39534714</td>\n",
       "      <td>953947559</td>\n",
       "      <td>953937444.0</td>\n",
       "      <td>Tornado outbreak of May 26–31, 2013</td>\n",
       "      <td>39534714</td>\n",
       "      <td>5906</td>\n",
       "      <td>6294</td>\n",
       "      <td>[ref, {{cite web|title=Verona man killed by fa...</td>\n",
       "      <td>[VB, WIKICODE, NN, NN, NN, NNP, NN, VBN, IN, V...</td>\n",
       "      <td>0.5984</td>\n",
       "      <td>CONS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      URL   tld  \\\n",
       "167129  http://www.katv.com/story/22467593/update-scot...  katv   \n",
       "\n",
       "                                                citations  \\\n",
       "167129  {{cite web|title=Scott County Sheriff drowns d...   \n",
       "\n",
       "                                                    Title         sections  \\\n",
       "167129  Scott County Sheriff drowns during rescue, 3 o...  Initial Section   \n",
       "\n",
       "       type_of_citation ID_list        id       r_id   r_parentid  \\\n",
       "167129         cite web    None  39534714  953947559  953937444.0   \n",
       "\n",
       "                                 page_title   page_id  ref_index  total_words  \\\n",
       "167129  Tornado outbreak of May 26–31, 2013  39534714       5906         6294   \n",
       "\n",
       "                                        neighboring_words  \\\n",
       "167129  [ref, {{cite web|title=Verona man killed by fa...   \n",
       "\n",
       "                                         neighboring_tags  bias_score label  \\\n",
       "167129  [VB, WIKICODE, NN, NN, NN, NNP, NN, VBN, IN, V...      0.5984  CONS   \n",
       "\n",
       "        label_category  \n",
       "167129               0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_with_features[dataset_with_features['label'] == 'CONS'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "mm0r1G8kQiRz"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>tld</th>\n",
       "      <th>citations</th>\n",
       "      <th>Title</th>\n",
       "      <th>sections</th>\n",
       "      <th>type_of_citation</th>\n",
       "      <th>ID_list</th>\n",
       "      <th>id</th>\n",
       "      <th>r_id</th>\n",
       "      <th>r_parentid</th>\n",
       "      <th>page_title</th>\n",
       "      <th>page_id</th>\n",
       "      <th>ref_index</th>\n",
       "      <th>total_words</th>\n",
       "      <th>neighboring_words</th>\n",
       "      <th>neighboring_tags</th>\n",
       "      <th>bias_score</th>\n",
       "      <th>label</th>\n",
       "      <th>label_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>515909</th>\n",
       "      <td>http://www.liberation.fr/culture/0101328757-or...</td>\n",
       "      <td>liberation</td>\n",
       "      <td>{{cite web| url=http://www.liberation.fr/cultu...</td>\n",
       "      <td>L'Orchestre Andalous d'Isra\\xebl r\\xe9unit mus...</td>\n",
       "      <td>Initial Section</td>\n",
       "      <td>cite web</td>\n",
       "      <td>None</td>\n",
       "      <td>23515089</td>\n",
       "      <td>925772300</td>\n",
       "      <td>917229142.0</td>\n",
       "      <td>Israeli Andalusian Orchestra</td>\n",
       "      <td>23515089</td>\n",
       "      <td>370</td>\n",
       "      <td>629</td>\n",
       "      <td>[web, url, http, :, www.jpost.com/ArtsAndCultu...</td>\n",
       "      <td>[NN, JJ, NN, :, NN, ., JJ, NN, :, NN, ., JJ, N...</td>\n",
       "      <td>-1.4031</td>\n",
       "      <td>LIBR</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      URL         tld  \\\n",
       "515909  http://www.liberation.fr/culture/0101328757-or...  liberation   \n",
       "\n",
       "                                                citations  \\\n",
       "515909  {{cite web| url=http://www.liberation.fr/cultu...   \n",
       "\n",
       "                                                    Title         sections  \\\n",
       "515909  L'Orchestre Andalous d'Isra\\xebl r\\xe9unit mus...  Initial Section   \n",
       "\n",
       "       type_of_citation ID_list        id       r_id   r_parentid  \\\n",
       "515909         cite web    None  23515089  925772300  917229142.0   \n",
       "\n",
       "                          page_title   page_id  ref_index  total_words  \\\n",
       "515909  Israeli Andalusian Orchestra  23515089        370          629   \n",
       "\n",
       "                                        neighboring_words  \\\n",
       "515909  [web, url, http, :, www.jpost.com/ArtsAndCultu...   \n",
       "\n",
       "                                         neighboring_tags  bias_score label  \\\n",
       "515909  [NN, JJ, NN, :, NN, ., JJ, NN, :, NN, ., JJ, N...     -1.4031  LIBR   \n",
       "\n",
       "        label_category  \n",
       "515909               1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_with_features[dataset_with_features['label'] == 'LIBR'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "YBK9hz_KQiR4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>tld</th>\n",
       "      <th>citations</th>\n",
       "      <th>Title</th>\n",
       "      <th>sections</th>\n",
       "      <th>type_of_citation</th>\n",
       "      <th>ID_list</th>\n",
       "      <th>id</th>\n",
       "      <th>r_id</th>\n",
       "      <th>r_parentid</th>\n",
       "      <th>page_title</th>\n",
       "      <th>page_id</th>\n",
       "      <th>ref_index</th>\n",
       "      <th>total_words</th>\n",
       "      <th>neighboring_words</th>\n",
       "      <th>neighboring_tags</th>\n",
       "      <th>bias_score</th>\n",
       "      <th>label</th>\n",
       "      <th>label_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>381685</th>\n",
       "      <td>https://www.manchestereveningnews.co.uk/whats-...</td>\n",
       "      <td>manchestereveningnews</td>\n",
       "      <td>{{Cite news|url=https://www.manchestereveningn...</td>\n",
       "      <td>Denise Welch strips off to star in Gary Barlow...</td>\n",
       "      <td>Early life and education</td>\n",
       "      <td>cite news</td>\n",
       "      <td>None</td>\n",
       "      <td>1423334</td>\n",
       "      <td>953871470</td>\n",
       "      <td>953504322.0</td>\n",
       "      <td>Fern Britton</td>\n",
       "      <td>1423334</td>\n",
       "      <td>2152</td>\n",
       "      <td>3967</td>\n",
       "      <td>[by, Mark, Davenport, called, ''Photoshopping,...</td>\n",
       "      <td>[IN, NNP, NNP, VBD, VBG, '', VBG, JJ, NN, NNP,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MODR</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      URL  \\\n",
       "381685  https://www.manchestereveningnews.co.uk/whats-...   \n",
       "\n",
       "                          tld  \\\n",
       "381685  manchestereveningnews   \n",
       "\n",
       "                                                citations  \\\n",
       "381685  {{Cite news|url=https://www.manchestereveningn...   \n",
       "\n",
       "                                                    Title  \\\n",
       "381685  Denise Welch strips off to star in Gary Barlow...   \n",
       "\n",
       "                        sections type_of_citation ID_list       id       r_id  \\\n",
       "381685  Early life and education        cite news    None  1423334  953871470   \n",
       "\n",
       "         r_parentid    page_title  page_id  ref_index  total_words  \\\n",
       "381685  953504322.0  Fern Britton  1423334       2152         3967   \n",
       "\n",
       "                                        neighboring_words  \\\n",
       "381685  [by, Mark, Davenport, called, ''Photoshopping,...   \n",
       "\n",
       "                                         neighboring_tags  bias_score label  \\\n",
       "381685  [IN, NNP, NNP, VBD, VBG, '', VBG, JJ, NN, NNP,...         0.0  MODR   \n",
       "\n",
       "        label_category  \n",
       "381685               2  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_with_features[dataset_with_features['label'] == 'MODR'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "D058r2hVQiSD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 630000/630000 [00:04<00:00, 150288.84it/s]\n"
     ]
    }
   ],
   "source": [
    "## Convert citations' text to UTF-8\n",
    "dataset_with_features['citations'] = dataset_with_features['citations'].progress_apply(lambda x: x.encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Pm0UMMhMQiSH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LIBR    210000\n",
       "CONS    210000\n",
       "MODR    210000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_with_features['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYZAVZtjQiSL"
   },
   "source": [
    "### Taking the unique `sections` and one hot encoding it to get a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "pEgn-G66QiSN"
   },
   "outputs": [],
   "source": [
    "# Only processing auxiliary features which are going to be used in the neural network\n",
    "auxiliary_features = dataset_with_features[\n",
    "    ['sections', 'citations', 'id', 'ref_index',\n",
    "     'total_words', 'neighboring_tags', 'label_category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "OM15qPnFQiSS"
   },
   "outputs": [],
   "source": [
    "auxiliary_features['sections'] = auxiliary_features['sections'].apply(\n",
    "    lambda x: x.encode('utf-8') if isinstance(x, str) else str(x))\n",
    "auxiliary_features['sections'] = auxiliary_features['sections'].astype(str)\n",
    "auxiliary_features['sections'] = auxiliary_features['sections'].apply(lambda x: x.split(', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "h00y47C6QiSY"
   },
   "outputs": [],
   "source": [
    "section_counts = pd.Series(Counter(chain.from_iterable(x for x in auxiliary_features.sections)))\n",
    "largest_sections = section_counts.nlargest(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "0u1M1zSAQiSg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 630000/630000 [00:03<00:00, 198912.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# Change section to `OTHERS` if occurence of the section is not in the 150 largest sections\n",
    "auxiliary_features['sections'] = auxiliary_features['sections'].progress_apply(\n",
    "    lambda x: list(set(['Others' if i not in largest_sections else i for i in x]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "HxMlOqTxQiSm"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sections</th>\n",
       "      <th>citations</th>\n",
       "      <th>id</th>\n",
       "      <th>ref_index</th>\n",
       "      <th>total_words</th>\n",
       "      <th>neighboring_tags</th>\n",
       "      <th>label_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>167129</th>\n",
       "      <td>[b'Initial Section']</td>\n",
       "      <td>b'{{cite web|title=Scott County Sheriff drowns...</td>\n",
       "      <td>39534714</td>\n",
       "      <td>5906</td>\n",
       "      <td>6294</td>\n",
       "      <td>[VB, WIKICODE, NN, NN, NN, NNP, NN, VBN, IN, V...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189891</th>\n",
       "      <td>[b'Initial Section']</td>\n",
       "      <td>b'{{cite book|last=Villard|first=Erik|title=Un...</td>\n",
       "      <td>21683511</td>\n",
       "      <td>1694</td>\n",
       "      <td>3346</td>\n",
       "      <td>[DT, NN, VBD, DT, CD, NNP, NN, NN, ., VB, JJ, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103624</th>\n",
       "      <td>[b'Initial Section']</td>\n",
       "      <td>b'{{cite web|url=http://www.cmt.com/news/17648...</td>\n",
       "      <td>47667920</td>\n",
       "      <td>23546</td>\n",
       "      <td>26675</td>\n",
       "      <td>[NNP, NNP, CD, NN, IN, DT, CD, JJS, VBN, IN, N...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65374</th>\n",
       "      <td>[b'Initial Section']</td>\n",
       "      <td>b'{{cite news|title=Auer secures 2020 BMW driv...</td>\n",
       "      <td>61096497</td>\n",
       "      <td>3391</td>\n",
       "      <td>4723</td>\n",
       "      <td>[NNP, NNP, NN, NNP, NNP, VBZ, TO, DT, NN, IN, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187981</th>\n",
       "      <td>[b'Initial Section']</td>\n",
       "      <td>b\"{{cite web|url=http://www.ynetnews.com/artic...</td>\n",
       "      <td>42184312</td>\n",
       "      <td>28319</td>\n",
       "      <td>39825</td>\n",
       "      <td>[RB, IN, NN, ., VB, WIKICODE, NN, NN, JJ, NN, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    sections  \\\n",
       "167129  [b'Initial Section']   \n",
       "189891  [b'Initial Section']   \n",
       "103624  [b'Initial Section']   \n",
       "65374   [b'Initial Section']   \n",
       "187981  [b'Initial Section']   \n",
       "\n",
       "                                                citations        id  \\\n",
       "167129  b'{{cite web|title=Scott County Sheriff drowns...  39534714   \n",
       "189891  b'{{cite book|last=Villard|first=Erik|title=Un...  21683511   \n",
       "103624  b'{{cite web|url=http://www.cmt.com/news/17648...  47667920   \n",
       "65374   b'{{cite news|title=Auer secures 2020 BMW driv...  61096497   \n",
       "187981  b\"{{cite web|url=http://www.ynetnews.com/artic...  42184312   \n",
       "\n",
       "        ref_index  total_words  \\\n",
       "167129       5906         6294   \n",
       "189891       1694         3346   \n",
       "103624      23546        26675   \n",
       "65374        3391         4723   \n",
       "187981      28319        39825   \n",
       "\n",
       "                                         neighboring_tags  label_category  \n",
       "167129  [VB, WIKICODE, NN, NN, NN, NNP, NN, VBN, IN, V...               0  \n",
       "189891  [DT, NN, VBD, DT, CD, NNP, NN, NN, ., VB, JJ, ...               0  \n",
       "103624  [NNP, NNP, CD, NN, IN, DT, CD, JJS, VBN, IN, N...               0  \n",
       "65374   [NNP, NNP, NN, NNP, NNP, VBZ, TO, DT, NN, IN, ...               0  \n",
       "187981  [RB, IN, NN, ., VB, WIKICODE, NN, NN, JJ, NN, ...               0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auxiliary_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "HgxnpoUXQiSp"
   },
   "outputs": [],
   "source": [
    "section_dummies = pd.get_dummies(auxiliary_features.sections.apply(pd.Series).stack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "AMtDBKLvQiSw"
   },
   "outputs": [],
   "source": [
    "auxiliary_features = auxiliary_features.join(section_dummies.sum(level=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "PeP8GmKJQiS2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citations</th>\n",
       "      <th>id</th>\n",
       "      <th>ref_index</th>\n",
       "      <th>total_words</th>\n",
       "      <th>neighboring_tags</th>\n",
       "      <th>label_category</th>\n",
       "      <th>Others</th>\n",
       "      <th>b'2000s'</th>\n",
       "      <th>b'2010s'</th>\n",
       "      <th>b'20th century'</th>\n",
       "      <th>...</th>\n",
       "      <th>b'Transactions'</th>\n",
       "      <th>b'Transfers'</th>\n",
       "      <th>b'U.S. House of Representatives'</th>\n",
       "      <th>b'United States'</th>\n",
       "      <th>b'Victims'</th>\n",
       "      <th>b'Videography'</th>\n",
       "      <th>b'Views'</th>\n",
       "      <th>b'Winners'</th>\n",
       "      <th>b'Work'</th>\n",
       "      <th>b'Works'</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>167129</th>\n",
       "      <td>b'{{cite web|title=Scott County Sheriff drowns...</td>\n",
       "      <td>39534714</td>\n",
       "      <td>5906</td>\n",
       "      <td>6294</td>\n",
       "      <td>[VB, WIKICODE, NN, NN, NN, NNP, NN, VBN, IN, V...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189891</th>\n",
       "      <td>b'{{cite book|last=Villard|first=Erik|title=Un...</td>\n",
       "      <td>21683511</td>\n",
       "      <td>1694</td>\n",
       "      <td>3346</td>\n",
       "      <td>[DT, NN, VBD, DT, CD, NNP, NN, NN, ., VB, JJ, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103624</th>\n",
       "      <td>b'{{cite web|url=http://www.cmt.com/news/17648...</td>\n",
       "      <td>47667920</td>\n",
       "      <td>23546</td>\n",
       "      <td>26675</td>\n",
       "      <td>[NNP, NNP, CD, NN, IN, DT, CD, JJS, VBN, IN, N...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65374</th>\n",
       "      <td>b'{{cite news|title=Auer secures 2020 BMW driv...</td>\n",
       "      <td>61096497</td>\n",
       "      <td>3391</td>\n",
       "      <td>4723</td>\n",
       "      <td>[NNP, NNP, NN, NNP, NNP, VBZ, TO, DT, NN, IN, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187981</th>\n",
       "      <td>b\"{{cite web|url=http://www.ynetnews.com/artic...</td>\n",
       "      <td>42184312</td>\n",
       "      <td>28319</td>\n",
       "      <td>39825</td>\n",
       "      <td>[RB, IN, NN, ., VB, WIKICODE, NN, NN, JJ, NN, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 157 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                citations        id  \\\n",
       "167129  b'{{cite web|title=Scott County Sheriff drowns...  39534714   \n",
       "189891  b'{{cite book|last=Villard|first=Erik|title=Un...  21683511   \n",
       "103624  b'{{cite web|url=http://www.cmt.com/news/17648...  47667920   \n",
       "65374   b'{{cite news|title=Auer secures 2020 BMW driv...  61096497   \n",
       "187981  b\"{{cite web|url=http://www.ynetnews.com/artic...  42184312   \n",
       "\n",
       "        ref_index  total_words  \\\n",
       "167129       5906         6294   \n",
       "189891       1694         3346   \n",
       "103624      23546        26675   \n",
       "65374        3391         4723   \n",
       "187981      28319        39825   \n",
       "\n",
       "                                         neighboring_tags  label_category  \\\n",
       "167129  [VB, WIKICODE, NN, NN, NN, NNP, NN, VBN, IN, V...               0   \n",
       "189891  [DT, NN, VBD, DT, CD, NNP, NN, NN, ., VB, JJ, ...               0   \n",
       "103624  [NNP, NNP, CD, NN, IN, DT, CD, JJS, VBN, IN, N...               0   \n",
       "65374   [NNP, NNP, NN, NNP, NNP, VBZ, TO, DT, NN, IN, ...               0   \n",
       "187981  [RB, IN, NN, ., VB, WIKICODE, NN, NN, JJ, NN, ...               0   \n",
       "\n",
       "        Others  b'2000s'  b'2010s'  b'20th century'  ...  b'Transactions'  \\\n",
       "167129       0         0         0                0  ...                0   \n",
       "189891       0         0         0                0  ...                0   \n",
       "103624       0         0         0                0  ...                0   \n",
       "65374        0         0         0                0  ...                0   \n",
       "187981       0         0         0                0  ...                0   \n",
       "\n",
       "        b'Transfers'  b'U.S. House of Representatives'  b'United States'  \\\n",
       "167129             0                                 0                 0   \n",
       "189891             0                                 0                 0   \n",
       "103624             0                                 0                 0   \n",
       "65374              0                                 0                 0   \n",
       "187981             0                                 0                 0   \n",
       "\n",
       "        b'Victims'  b'Videography'  b'Views'  b'Winners'  b'Work'  b'Works'  \n",
       "167129           0               0         0           0        0         0  \n",
       "189891           0               0         0           0        0         0  \n",
       "103624           0               0         0           0        0         0  \n",
       "65374            0               0         0           0        0         0  \n",
       "187981           0               0         0           0        0         0  \n",
       "\n",
       "[5 rows x 157 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auxiliary_features.drop('sections', axis=1, inplace=True)\n",
    "auxiliary_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnTCM3XlQiTA"
   },
   "source": [
    "As we can see for the feature `total_number_of_words`, the mean and median **(since it is more robust in nature!)** are pretty high for articles which are liberal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Kk5izWZ_QiTC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mean length of liberal articles: 11279.110685714286\n",
      "Total median length of liberal articles: 4996.0\n"
     ]
    }
   ],
   "source": [
    "print('Total mean length of liberal articles: {}'.format( ## liberal articles are longer\n",
    "    auxiliary_features[auxiliary_features['label_category'] == 1]['total_words'].mean()))\n",
    "print('Total median length of liberal articles: {}'.format(\n",
    "    auxiliary_features[auxiliary_features['label_category'] == 1]['total_words'].median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "21mjdj1uQiTH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mean length of moderate articles: 10091.52959047619\n",
      "Total median length of moderate articles: 3823.0\n"
     ]
    }
   ],
   "source": [
    "print('Total mean length of moderate articles: {}'.format( ## Moderate articles in general have a shorter length\n",
    "    auxiliary_features[auxiliary_features['label_category'] == 2]['total_words'].mean()))\n",
    "print('Total median length of moderate articles: {}'.format(\n",
    "    auxiliary_features[auxiliary_features['label_category'] == 2]['total_words'].median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ux3TznVAQiTN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mean length of conservative articles: 11413.3415\n",
      "Total median length of conservative articles: 4587.0\n"
     ]
    }
   ],
   "source": [
    "print('Total mean length of conservative articles: {}'.format( ## slightly smaller than liberal\n",
    "    auxiliary_features[auxiliary_features['label_category'] == 0]['total_words'].mean()))\n",
    "print('Total median length of conservative articles: {}'.format(\n",
    "    auxiliary_features[auxiliary_features['label_category'] == 0]['total_words'].median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdH9m5NsQiTX"
   },
   "source": [
    "### Taking the `neighboring_tags` and making an encoder dictionary for it\n",
    "\n",
    "To have more info about how what tag mean what: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "l0Pm-AZzQiTY"
   },
   "outputs": [],
   "source": [
    "citation_tag_features = dataset_with_features[['id', 'citations', 'neighboring_tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "GDSbMyBFQiTe"
   },
   "outputs": [],
   "source": [
    "# citation_tag_features['neighboring_tags'] = citation_tag_features['neighboring_tags'].progress_apply(\n",
    "#     lambda x: x.replace(\"'\", \"\").replace('[', '').replace(']', '').replace('\\n', '').split(' ')\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "OSZ4lTBJQiTl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DT', 'NN', 'VBD', 'DT', 'CD', 'NNP', 'NN', 'NN', '.', 'VB'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citation_tag_features.iloc[1]['neighboring_tags'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "2KwXoEEuQiT7"
   },
   "outputs": [],
   "source": [
    "# Get the count for each POS tag so that we have an estimation as to how many are there\n",
    "tag_counts = pd.Series(Counter(chain.from_iterable(x for x in citation_tag_features.neighboring_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "4-IJJgbSQiT-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LS        1\n",
       "``      189\n",
       "WP$    1006\n",
       "SYM    1063\n",
       "UH     1238\n",
       "PDT    2216\n",
       "$      2982\n",
       "RBS    5880\n",
       "EX     5936\n",
       "RBR    7639\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Considering the 10 smallest tags and checking which one does not have resemblance\n",
    "tag_counts.nsmallest(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "XKkKvklZQiUF"
   },
   "outputs": [],
   "source": [
    "# tag_counts.to_csv('/dlabdata1/harshdee/tag_counts.csv', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DncrnDVgQiUI"
   },
   "source": [
    "We are going to replace `LS`, `the 2 backquotes` and the `the dollar symbol` since they do not have too much use case and do not give too much information about the context of the neighboring citation text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "ml-H0ccVQiUI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 630000/630000 [00:07<00:00, 88101.54it/s] \n"
     ]
    }
   ],
   "source": [
    "OTHER_TAGS = ['LS', '``', '$']\n",
    "citation_tag_features['neighboring_tags'] = citation_tag_features['neighboring_tags'].progress_apply(\n",
    "    lambda x: [i if i not in OTHER_TAGS else 'Others' for i in x]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEKzxc_5QiUL"
   },
   "source": [
    "Now, we can use the `count vectorizer` to represent the `POS tags` as a vector where each element of the vector represents the count of that tag in that particular citation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "4OfJ0ogNQiUL"
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer() # Instantiate the vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Ds9BRYJZQiUP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 630000/630000 [00:01<00:00, 368925.50it/s]\n"
     ]
    }
   ],
   "source": [
    "citation_tag_features['neighboring_tags'] = citation_tag_features['neighboring_tags'].progress_apply(\n",
    "    lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "0CoQn3UWQiUW"
   },
   "outputs": [],
   "source": [
    "transformed_neighboring_tags = cv.fit_transform(citation_tag_features['neighboring_tags'])\n",
    "transformed_neighboring_tags = pd.DataFrame(transformed_neighboring_tags.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "oAzwsZWJQiUb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>citations</th>\n",
       "      <th>neighboring_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>167129</th>\n",
       "      <td>39534714</td>\n",
       "      <td>b'{{cite web|title=Scott County Sheriff drowns...</td>\n",
       "      <td>VB WIKICODE NN NN NN NNP NN VBN IN VBG NN IN N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189891</th>\n",
       "      <td>21683511</td>\n",
       "      <td>b'{{cite book|last=Villard|first=Erik|title=Un...</td>\n",
       "      <td>DT NN VBD DT CD NNP NN NN . VB JJ NN VBN NNPS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103624</th>\n",
       "      <td>47667920</td>\n",
       "      <td>b'{{cite web|url=http://www.cmt.com/news/17648...</td>\n",
       "      <td>NNP NNP CD NN IN DT CD JJS VBN IN NNP NNP TO N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65374</th>\n",
       "      <td>61096497</td>\n",
       "      <td>b'{{cite news|title=Auer secures 2020 BMW driv...</td>\n",
       "      <td>NNP NNP NN NNP NNP VBZ TO DT NN IN NNP NNP NNP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187981</th>\n",
       "      <td>42184312</td>\n",
       "      <td>b\"{{cite web|url=http://www.ynetnews.com/artic...</td>\n",
       "      <td>RB IN NN . VB WIKICODE NN NN JJ NN : JJ NN : J...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                          citations  \\\n",
       "167129  39534714  b'{{cite web|title=Scott County Sheriff drowns...   \n",
       "189891  21683511  b'{{cite book|last=Villard|first=Erik|title=Un...   \n",
       "103624  47667920  b'{{cite web|url=http://www.cmt.com/news/17648...   \n",
       "65374   61096497  b'{{cite news|title=Auer secures 2020 BMW driv...   \n",
       "187981  42184312  b\"{{cite web|url=http://www.ynetnews.com/artic...   \n",
       "\n",
       "                                         neighboring_tags  \n",
       "167129  VB WIKICODE NN NN NN NNP NN VBN IN VBG NN IN N...  \n",
       "189891  DT NN VBD DT CD NNP NN NN . VB JJ NN VBN NNPS ...  \n",
       "103624  NNP NNP CD NN IN DT CD JJS VBN IN NNP NNP TO N...  \n",
       "65374   NNP NNP NN NNP NNP VBZ TO DT NN IN NNP NNP NNP...  \n",
       "187981  RB IN NN . VB WIKICODE NN NN JJ NN : JJ NN : J...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citation_tag_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "dpou46xoQiUh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((630000, 35), (630000, 3))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_neighboring_tags.shape, citation_tag_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "xy4RduneQiU6"
   },
   "outputs": [],
   "source": [
    "citation_tag_features = citation_tag_features.reset_index(drop=True)\n",
    "citation_tag_features = pd.concat([citation_tag_features, transformed_neighboring_tags], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "H4705kgFQiVK"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>citations</th>\n",
       "      <th>cc</th>\n",
       "      <th>cd</th>\n",
       "      <th>dt</th>\n",
       "      <th>ex</th>\n",
       "      <th>fw</th>\n",
       "      <th>in</th>\n",
       "      <th>jj</th>\n",
       "      <th>jjr</th>\n",
       "      <th>...</th>\n",
       "      <th>vb</th>\n",
       "      <th>vbd</th>\n",
       "      <th>vbg</th>\n",
       "      <th>vbn</th>\n",
       "      <th>vbp</th>\n",
       "      <th>vbz</th>\n",
       "      <th>wdt</th>\n",
       "      <th>wikicode</th>\n",
       "      <th>wp</th>\n",
       "      <th>wrb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39534714</td>\n",
       "      <td>b'{{cite web|title=Scott County Sheriff drowns...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21683511</td>\n",
       "      <td>b'{{cite book|last=Villard|first=Erik|title=Un...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47667920</td>\n",
       "      <td>b'{{cite web|url=http://www.cmt.com/news/17648...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61096497</td>\n",
       "      <td>b'{{cite news|title=Auer secures 2020 BMW driv...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42184312</td>\n",
       "      <td>b\"{{cite web|url=http://www.ynetnews.com/artic...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                          citations  cc  cd  dt  \\\n",
       "0  39534714  b'{{cite web|title=Scott County Sheriff drowns...   0   2   0   \n",
       "1  21683511  b'{{cite book|last=Villard|first=Erik|title=Un...   0   3   3   \n",
       "2  47667920  b'{{cite web|url=http://www.cmt.com/news/17648...   1   5   3   \n",
       "3  61096497  b'{{cite news|title=Auer secures 2020 BMW driv...   0   1   3   \n",
       "4  42184312  b\"{{cite web|url=http://www.ynetnews.com/artic...   0   2   1   \n",
       "\n",
       "   ex  fw  in  jj  jjr  ...  vb  vbd  vbg  vbn  vbp  vbz  wdt  wikicode  wp  \\\n",
       "0   0   0   3   3    0  ...   2    1    2    1    0    0    0         1   0   \n",
       "1   0   0   2   3    0  ...   1    1    1    1    0    0    0         0   0   \n",
       "2   0   0   6   1    0  ...   1    0    0    2    0    0    0         0   0   \n",
       "3   0   0   4   1    0  ...   2    1    1    0    0    2    0         0   1   \n",
       "4   0   0   4   3    0  ...   2    2    0    0    0    1    0         1   0   \n",
       "\n",
       "   wrb  \n",
       "0    0  \n",
       "1    0  \n",
       "2    0  \n",
       "3    0  \n",
       "4    0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citation_tag_features.drop('neighboring_tags', axis=1, inplace=True)\n",
    "citation_tag_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rXm4dTiQiVP"
   },
   "source": [
    "## Features for the LSTM - more time sequence related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tYHz_a7QiVQ"
   },
   "source": [
    "### Citation's original text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "3KvIpJ6bQiVR"
   },
   "outputs": [],
   "source": [
    "# Create a separate dataframe for preprocessing citation text\n",
    "citation_text_features = dataset_with_features[['id', 'citations', 'label_category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TskCSDyuQiVa",
    "outputId": "9a6daf27-6573-427f-cae7-8e942a5e41e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 630000/630000 [00:13<00:00, 47304.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert the citation into a list by breaking it down into characters\n",
    "citation_text_features['characters'] = citation_text_features['citations'].progress_apply(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i4EKn5fDQiVe",
    "outputId": "5a84f2fa-f484-4236-d0c7-4003f66ba544"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([123,  99, 105, 116, 101,  32, 119,  98, 124, 108,  61,  83, 111,\n",
       "             67, 117, 110, 121, 104, 114, 102, 100, 115, 103,  44,  51, 109,\n",
       "            112,  58,  47,  46, 107,  97, 118,  50,  52,  54,  55,  53,  57,\n",
       "             45,  75,  65,  84,  86,  74,  49,  48, 125,  69,  85,  79,  56,\n",
       "             77,  72, 120,  68,  66,  87,  91,  78,  93,  76,  73,  39,  80,\n",
       "            106,  71,  82,  95,  70,  89,  92, 113,  37,  36,  63, 122,  35,\n",
       "             38,  34,  59,  90,  40,  41,  33,  88,  43,  62,  81,  60, 126,\n",
       "             64,  42,  96,  94],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the character counts for each unique character\n",
    "char_counts = pd.Series(Counter(chain.from_iterable(x for x in citation_text_features.characters)))\n",
    "char_counts.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kFGYdx4WQiVp",
    "outputId": "d16cd81a-677b-4c15-eb51-fec479c1d5a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max length of the longest citation in terms of characters is: 10057\n",
      "The mean length of the longest citation in terms of characters is: 276.183353968254\n",
      "The median length of the longest citation in terms of characters is: 257.0\n"
     ]
    }
   ],
   "source": [
    "print('The max length of the longest citation in terms of characters is: {}'.format(\n",
    "    max(citation_text_features.characters.apply(lambda x: len(x)))))\n",
    "\n",
    "print('The mean length of the longest citation in terms of characters is: {}'.format(\n",
    "    citation_text_features.characters.apply(lambda x: len(x)).mean()))\n",
    "\n",
    "print('The median length of the longest citation in terms of characters is: {}'.format(\n",
    "    citation_text_features.characters.apply(lambda x: len(x)).median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "qyfMEnGDQiVx"
   },
   "outputs": [],
   "source": [
    "# Make a dictionary for creating a mapping between the char and the corresponding index\n",
    "char2ind = {char: i for i, char in enumerate(char_counts.index)}\n",
    "ind2char = {i: char for i, char in enumerate(char_counts.index)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "_2c9FEMJQiV6"
   },
   "outputs": [],
   "source": [
    "# Map each character into the citation to its corresponding index and store it in a list\n",
    "X_char = []\n",
    "for citation in citation_text_features.citations:\n",
    "    citation_chars = []\n",
    "    for character in citation:\n",
    "        citation_chars.append(char2ind[character])\n",
    "        \n",
    "    X_char.append(citation_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9mwkFpKQiV-"
   },
   "source": [
    "Since the median length of the citation is 276, we have padded the input till 400 to get extra information which would be fed into the character embedding neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "ZuMbZQDBQiV_"
   },
   "outputs": [],
   "source": [
    "X_char = pad_sequences(X_char, maxlen=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1EuQgzGMQiWD",
    "outputId": "82aea44d-2e19-4c63-9acf-176edab68bed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 630000/630000 [01:21<00:00, 7734.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# Append the citation character list with their corresponding lists for making a dataset\n",
    "# for getting the character embeddings\n",
    "data = []\n",
    "for i in tqdm(range(len(X_char))):\n",
    "    data.append((X_char[i], int(citation_text_features.iloc[i]['label_category'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "aiAT0FEgQiWi"
   },
   "outputs": [],
   "source": [
    "# # Separate out the training data and labels for further verification use\n",
    "features = [i[0] for i in data]\n",
    "labels = [i[1] for i in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "iQnF_KVkQiWw"
   },
   "outputs": [],
   "source": [
    "## Splitting the data into training and testing\n",
    "training_data, testing_data, training_labels, testing_labels = train_test_split(\n",
    "    features, labels, train_size=0.9, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fm4r5Wf7QiXK"
   },
   "source": [
    "We are going to feed in the 400 character input since our median length comes out to be approximately 282 and train it on different labels to get the embedding or the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "aW85iKaJQiXK"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "categorical_labels = to_categorical(training_labels, num_classes=3)\n",
    "categorical_test_labels = to_categorical(testing_labels, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "swC-pK0bQiXP"
   },
   "outputs": [],
   "source": [
    "def citation_embedding_model():\n",
    "    \"\"\"\n",
    "    Citation embedding generator model where the dimension of the embedding is 50.\n",
    "    \"\"\"\n",
    "    main_input = Input(shape=(300, ), name='characters')\n",
    "    # input dim is basically the vocab size\n",
    "    emb = Embedding(input_dim=95, output_dim = 150, name='citation_embedding')(main_input)\n",
    "    rnn = Bidirectional(LSTM(40))\n",
    "    x = rnn(emb)\n",
    "    y = Dense(20, activation='sigmoid')(x)\n",
    "    de = Dense(3, activation='softmax')(y)\n",
    "    model = Model(inputs = main_input, outputs = de)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "mQcwlPYjQiXc"
   },
   "outputs": [],
   "source": [
    "# Instantiate the model and generate the summary\n",
    "model = citation_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mSjj4RolQiXh",
    "outputId": "5081c802-f5af-4b20-bfcd-6f0473d83e6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "characters (InputLayer)      [(None, 300)]             0         \n",
      "_________________________________________________________________\n",
      "citation_embedding (Embeddin (None, 300, 150)          14250     \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 80)                61120     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 20)                1620      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 77,053\n",
      "Trainable params: 77,053\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "lIUZTn8EQiXs"
   },
   "outputs": [],
   "source": [
    "def generator(features, labels, batch_size):\n",
    "    \"\"\"\n",
    "    Generator to create batches of data so that processing is easy.\n",
    "    \n",
    "    :param: features: the features of the model.\n",
    "    :param: labels: the labels of the model.\n",
    "    :param: batch_size: the size of the batch\n",
    "    \"\"\"\n",
    "    # Create empty arrays to contain batch of features and labels\n",
    "    batch_features = np.zeros((batch_size, 300))\n",
    "    batch_labels = np.zeros((batch_size, 3))\n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            # choose random index in features\n",
    "            index = np.random.choice(len(features), 1)[0]\n",
    "            batch_features[i] = features[index]\n",
    "            batch_labels[i] = categorical_labels[index]\n",
    "        yield batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OqvlKgIFQiX2",
    "outputId": "6049304c-e0e7-406c-abfe-71fd16c07284"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-58-35dfdbc42282>:4: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/30\n",
      "500/500 [==============================] - 103s 206ms/step - loss: 1.0410 - accuracy: 0.4270\n",
      "Epoch 2/30\n",
      "500/500 [==============================] - 103s 206ms/step - loss: 0.9966 - accuracy: 0.4841\n",
      "Epoch 3/30\n",
      "500/500 [==============================] - 103s 207ms/step - loss: 1.0216 - accuracy: 0.4791\n",
      "Epoch 4/30\n",
      "500/500 [==============================] - 104s 207ms/step - loss: 0.9903 - accuracy: 0.4987\n",
      "Epoch 5/30\n",
      "500/500 [==============================] - 104s 207ms/step - loss: 0.9860 - accuracy: 0.5093\n",
      "Epoch 6/30\n",
      "500/500 [==============================] - 104s 208ms/step - loss: 0.9303 - accuracy: 0.5549\n",
      "Epoch 7/30\n",
      "500/500 [==============================] - 103s 207ms/step - loss: 0.9620 - accuracy: 0.5463\n",
      "Epoch 8/30\n",
      "500/500 [==============================] - 103s 207ms/step - loss: 1.0163 - accuracy: 0.4783\n",
      "Epoch 9/30\n",
      "500/500 [==============================] - 102s 205ms/step - loss: 0.9599 - accuracy: 0.5351\n",
      "Epoch 10/30\n",
      "500/500 [==============================] - 104s 207ms/step - loss: 1.0288 - accuracy: 0.4502\n",
      "Epoch 11/30\n",
      "500/500 [==============================] - 104s 208ms/step - loss: 0.9894 - accuracy: 0.4771\n",
      "Epoch 12/30\n",
      "500/500 [==============================] - 105s 210ms/step - loss: 0.9429 - accuracy: 0.5094\n",
      "Epoch 13/30\n",
      "500/500 [==============================] - 105s 210ms/step - loss: 0.8737 - accuracy: 0.5814\n",
      "Epoch 14/30\n",
      "500/500 [==============================] - 105s 209ms/step - loss: 0.7745 - accuracy: 0.6388\n",
      "Epoch 15/30\n",
      "500/500 [==============================] - 105s 210ms/step - loss: 0.7300 - accuracy: 0.6640\n",
      "Epoch 16/30\n",
      "500/500 [==============================] - 105s 210ms/step - loss: 0.6300 - accuracy: 0.7173\n",
      "Epoch 17/30\n",
      "500/500 [==============================] - 104s 208ms/step - loss: 0.5003 - accuracy: 0.7875\n",
      "Epoch 18/30\n",
      "500/500 [==============================] - 104s 207ms/step - loss: 0.3969 - accuracy: 0.8364\n",
      "Epoch 19/30\n",
      "500/500 [==============================] - 103s 206ms/step - loss: 0.3365 - accuracy: 0.8661\n",
      "Epoch 20/30\n",
      "500/500 [==============================] - 103s 206ms/step - loss: 0.2899 - accuracy: 0.8877\n",
      "Epoch 21/30\n",
      "500/500 [==============================] - 103s 206ms/step - loss: 0.2578 - accuracy: 0.9034\n",
      "Epoch 22/30\n",
      "500/500 [==============================] - 103s 206ms/step - loss: 0.2342 - accuracy: 0.9138\n",
      "Epoch 23/30\n",
      "500/500 [==============================] - 103s 205ms/step - loss: 0.2168 - accuracy: 0.9210\n",
      "Epoch 24/30\n",
      "500/500 [==============================] - 103s 206ms/step - loss: 0.2007 - accuracy: 0.9279\n",
      "Epoch 25/30\n",
      "500/500 [==============================] - 103s 206ms/step - loss: 0.1910 - accuracy: 0.9320\n",
      "Epoch 26/30\n",
      "500/500 [==============================] - 102s 205ms/step - loss: 0.1788 - accuracy: 0.9373\n",
      "Epoch 27/30\n",
      "500/500 [==============================] - 103s 205ms/step - loss: 0.1699 - accuracy: 0.9402\n",
      "Epoch 28/30\n",
      "500/500 [==============================] - 103s 206ms/step - loss: 0.1637 - accuracy: 0.9427\n",
      "Epoch 29/30\n",
      "500/500 [==============================] - 103s 206ms/step - loss: 0.1585 - accuracy: 0.9444\n",
      "Epoch 30/30\n",
      "500/500 [==============================] - 103s 207ms/step - loss: 0.1505 - accuracy: 0.9474\n"
     ]
    }
   ],
   "source": [
    "# # Run the model with the data being generated by the generator with a batch size of 64\n",
    "# # and number of epochs to be set to 15\n",
    "hist = model.fit_generator(\n",
    "    generator(training_data, categorical_labels, 1024), steps_per_epoch=500, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BRTMyhQmQiX8",
    "outputId": "8de41381-5a03-4747-9e7e-b6f58e3d3503"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9430793650793651"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Evaluation of embedding model\n",
    "y_predicted_proba = model.predict(np.array(testing_data))\n",
    "predicted_class = np.argmax(y_predicted_proba, axis=1)\n",
    "accuracy_score(testing_labels, predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "RYyrlh-5QiYA"
   },
   "outputs": [],
   "source": [
    "# # Save the model so that we can retrieve it later\n",
    "# model.save('/content/sample_data/embedding_bias_model2.h5')\n",
    "from keras.models import load_model\n",
    "model = load_model('./embedding_bias_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9iV9IirbQiYJ",
    "outputId": "08acb1d2-7fd3-426c-9f0a-1248889b40c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95, 150)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Get the `citation_embedding` layer and get the weights for each character\n",
    "citation_layer = model.get_layer('citation_embedding')\n",
    "citation_weights = citation_layer.get_weights()[0]\n",
    "citation_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aylowJ28QiYQ",
    "outputId": "091391b2-c69d-4be7-9173-17a28d6a8200"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01804133, -0.03245307, -0.12361734,  0.04764726, -0.1037605 ,\n",
       "       -0.18249515, -0.00202649, -0.02972538, -0.02778249,  0.04539751,\n",
       "       -0.16687357,  0.00735664,  0.00089882,  0.10340928, -0.06338792,\n",
       "        0.01505945, -0.0854198 ,  0.00249107, -0.21004422,  0.03546045,\n",
       "        0.01065212,  0.00940321, -0.13779655, -0.02447194,  0.05968909,\n",
       "        0.03105203, -0.01691033, -0.05628253, -0.02027912,  0.01662555,\n",
       "       -0.14528242, -0.11639705, -0.0670182 , -0.07386018,  0.00341635,\n",
       "       -0.2526719 ,  0.02036618,  0.06978438, -0.00052654,  0.06041639,\n",
       "       -0.02310811,  0.12577496, -0.0981093 , -0.0460665 ,  0.07024252,\n",
       "        0.1767161 ,  0.11854591, -0.25253728,  0.1438085 ,  0.00949295,\n",
       "        0.01013856,  0.02204646,  0.1104747 ,  0.26085848,  0.08808202,\n",
       "        0.21609005, -0.0105921 , -0.25423908, -0.09452852, -0.21711965,\n",
       "        0.01737843, -0.17467532, -0.13874163, -0.3061189 ,  0.08532184,\n",
       "       -0.0952365 ,  0.00962504, -0.03715321, -0.05805329, -0.17805931,\n",
       "        0.05282052, -0.22631939,  0.09978569, -0.07213827, -0.10445875,\n",
       "       -0.04253771, -0.06361059, -0.14731504,  0.07000852, -0.16345447,\n",
       "        0.17090082,  0.0507946 ,  0.056885  ,  0.06575891,  0.17389166,\n",
       "       -0.06296387, -0.06144552, -0.02511318, -0.04402428,  0.02761464,\n",
       "        0.03367646,  0.03247438,  0.13904127, -0.23856519, -0.15551788,\n",
       "        0.02130155,  0.08747132, -0.23864047,  0.16339964, -0.15491812],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example of the first element of an embedding\n",
    "citation_weights[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xw8nga_QQiYU",
    "outputId": "16828d67-ab06-4d89-e7a9-6530cd55a752"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 630000/630000 [05:34<00:00, 1880.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# # Map the embedding of each character to the character in each corresponding citation and aggregate (sum)\n",
    "citation_text_features['embedding'] = citation_text_features['characters'].progress_apply(\n",
    "    lambda x: sum([citation_weights[char2ind[c]] for c in x])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4SO1yV-QiYY",
    "outputId": "7ee62a8e-48c5-4c97-c13a-38a0688e3520"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 630000/630000 [00:29<00:00, 21603.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# # Normalize the citation embeddings so that we can check for their similarity later\n",
    "citation_text_features['embedding'] = citation_text_features['embedding'].progress_apply(\n",
    "    lambda x: x/ np.linalg.norm(x, axis=0).reshape((-1, 1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j-dvGGwLQiYd",
    "outputId": "77fdae32-8a7f-40e0-f231-85718fc2c119"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Make the sum of the embedding to be summed up to 1\n",
    "np.sum(np.square(citation_text_features['embedding'].iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3PuN02TQiYh"
   },
   "source": [
    "### FastText embeddings for neighboring words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "u_sLatwWoZNc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del citations_bias_features\n",
    "del section_dummies\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "d_Fi0v3cQiYh"
   },
   "outputs": [],
   "source": [
    "# Load the pretrained embedding model on wikipedia\n",
    "model = FastText.load_fasttext_format('./wiki.en.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "I43AayQcQiYk"
   },
   "outputs": [],
   "source": [
    "# Create a separate dataframe for preprocessing citation words\n",
    "citation_word_features = dataset_with_features[['id', 'citations', 'neighboring_words', 'label_category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "j1LweDGWQiYp",
    "outputId": "92f29474-70e2-4aeb-8160-bc10fdcce6fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 630000/630000 [01:11<00:00, 8792.15it/s] \n"
     ]
    }
   ],
   "source": [
    "# Lowercase all the neighboring words for each of the citations\n",
    "citation_word_features['neighboring_words'] = citation_word_features['neighboring_words'].progress_apply(\n",
    "    lambda x: [i.lower() for i in x]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hotHvoa0QiYr"
   },
   "source": [
    "Get the total unique words with their respective counts in the total dataset. This is done in order to remove words which are of low frequency and will potentially act as noise to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "FyoNzVwnQiYr"
   },
   "outputs": [],
   "source": [
    "word_counts = pd.Series(Counter(chain.from_iterable(x for x in citation_word_features.neighboring_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "kiEN1wD9QiY4",
    "outputId": "a1658178-afbb-4d99-9fc0-659bb0dedff6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 1710005\n",
      "Total number of words whose occurence is less than 4: 1573563\n",
      "Difference: 136442\n"
     ]
    }
   ],
   "source": [
    "threshold = 4\n",
    "\n",
    "x = len(word_counts)\n",
    "y = len(word_counts[word_counts <= threshold])\n",
    "print('Total words: {}\\nTotal number of words whose occurence is less than 4: {}\\nDifference: {}'.format(x, y, x-y))\n",
    "words_less_than_threshold = word_counts[word_counts <= threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "ClfXLpv2QiY7",
    "outputId": "c37189ea-39fe-4a49-ef02-7f220a3be67d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 630000/630000 [00:46<00:00, 13530.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# Remove the words which have a count of less than 4 and replace them with the unique <UNK> symbol\n",
    "citation_word_features['neighboring_words'] = citation_word_features['neighboring_words'].progress_apply(\n",
    "    lambda x: [i if i not in words_less_than_threshold else '<UNK>' for i in x]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "z9D7jdrhQiY_"
   },
   "outputs": [],
   "source": [
    "# creating a mapping between word and index or vice versa\n",
    "words = pd.Series(Counter(chain.from_iterable(x for x in citation_word_features.neighboring_words))).index\n",
    "word2ind = {w: i for i, w in enumerate(words)}\n",
    "ind2words = {i: w for i, w in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "PW6p8osKQiZC",
    "outputId": "e80b9393-ce26-42a1-da74-ed2295ff34ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 136443/136443 [14:44<00:00, 154.26it/s]\n"
     ]
    }
   ],
   "source": [
    "word_embedding_matrix = np.zeros((len(word2ind), 300))\n",
    "for w in tqdm(word2ind):\n",
    "    index = word2ind[w]\n",
    "    word_embedding_matrix[index] = model.wv[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQzbCR3qQiZF"
   },
   "source": [
    "Once we have the word embedding for each word in the neighboring words, we sum the embeddings for each word together in neighboring words to get an embedding which represents the past 40 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "j9YVE95sQiZF",
    "outputId": "f808f69e-55da-4f19-9b03-23043c041e2a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 630000/630000 [01:18<00:00, 8046.97it/s] \n"
     ]
    }
   ],
   "source": [
    "citation_word_features['words_embedding'] = citation_word_features['neighboring_words'].progress_apply(\n",
    "    lambda x: sum([word_embedding_matrix[word2ind[w]] for w in x])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxA19n4OQiZJ"
   },
   "source": [
    "Now we have the `citation_word_features` and `citation_tag_features`, so we can join them together to form `time_sequence_features` which would be fed later into the LSTM.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "2pCFzXS7QiZJ"
   },
   "outputs": [],
   "source": [
    "# Join time sequence features with the citations dataset\n",
    "time_sequence_features = pd.concat([citation_tag_features, citation_word_features.reset_index(drop=True)], keys=['id', 'citations'], axis=1)\n",
    "time_sequence_features = time_sequence_features.loc[:, ~time_sequence_features.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "hRKESnEDQiZM",
    "outputId": "2bacfbaa-6f02-4cda-9060-52647afb7a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples in time features are: (630000, 42)\n"
     ]
    }
   ],
   "source": [
    "print('Total number of samples in time features are: {}'.format(time_sequence_features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "yDMqQV0XQiZi"
   },
   "outputs": [],
   "source": [
    "# citation_text = auxiliary_features.iloc[:,0]\n",
    "# auxiliary_features['citation_text'] = citation_text\n",
    "# auxiliary_features.drop('citation', axis=1, inplace=True)\n",
    "# auxiliary_features.rename({'citation_text': 'citation'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "GcEMY2wdQiZk",
    "outputId": "ada5c253-7f16-4ce5-c69b-1820c73d73af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(630000, 159)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join auxiliary features with the citations dataset\n",
    "citation_text_features.reset_index(drop=True, inplace=True)\n",
    "auxiliary_features.reset_index(drop=True, inplace=True)\n",
    "\n",
    "auxiliary_features = pd.concat([auxiliary_features, citation_text_features], keys=['id', 'citations'], axis=1)\n",
    "auxiliary_features = pd.concat([auxiliary_features['citations'], auxiliary_features['id']], axis=1)\n",
    "auxiliary_features = auxiliary_features.loc[:, ~auxiliary_features.columns.duplicated()]\n",
    "auxiliary_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "76uyTPy7QiZm"
   },
   "outputs": [],
   "source": [
    "# Drop columns with are duplicates\n",
    "auxiliary_features.drop(['neighboring_tags', 'characters'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "lFBdQRNJQiZp",
    "outputId": "6b0c6d22-b75b-48fa-e8c1-4f05225ebbb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del word_embedding_matrix\n",
    "del citation_word_features\n",
    "del citation_text_features\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zi5d0kjlQiZt"
   },
   "source": [
    "## Making sets for `auxiliary` and `time sequence` features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "-cbIK664QiZu"
   },
   "outputs": [],
   "source": [
    "data = dataset_with_features[['id', 'citations', 'label_category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "3ATrMEq7QiZz"
   },
   "outputs": [],
   "source": [
    "# Join the time sequence features for the data\n",
    "time_sequence_features = pd.concat([time_sequence_features['id'], time_sequence_features['citations']], axis=1)\n",
    "time_sequence_features = pd.concat([time_sequence_features, data.reset_index(drop=True)], keys=['id', 'citations'], axis=1)\n",
    "time_sequence_features.columns = time_sequence_features.columns.droplevel(0)\n",
    "time_sequence_features = time_sequence_features.loc[:, ~time_sequence_features.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "HugG1GIjQiZ2",
    "outputId": "191c6459-be4e-4187-a45d-6b9c655cdf62"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 630000/630000 [02:11<00:00, 4784.79it/s] \n"
     ]
    }
   ],
   "source": [
    "time_sequence_features['words_embedding'] = time_sequence_features['words_embedding'].progress_apply(\n",
    "    lambda x: x.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "ByO4U8ZaQiZ8",
    "outputId": "9d59c313-0739-4d97-de0c-f8dee764c572"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 630000/630000 [06:58<00:00, 1503.78it/s] \n"
     ]
    }
   ],
   "source": [
    "auxiliary_features['embedding'] = auxiliary_features['embedding'].progress_apply(lambda x: x.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "pxdyGVxFQiZ-",
    "outputId": "069e18a6-30a0-455f-8421-b4702ebe661c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(630000, 630000)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(time_sequence_features), len(auxiliary_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "ul0MOfXOQiaA",
    "outputId": "10b6b159-b54b-4a45-a2a7-722f83f489ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEV9WDWaQiaG"
   },
   "source": [
    "## Splitting the dataset into training, testing and validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ufg5abMQiaH"
   },
   "source": [
    "The split is done into 80-10-10 ratio so that we have more training data to train on and have validation dataset to make sure that the model is working as anticipated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "L8r2_UscQiaH",
    "outputId": "9ebf3117-877f-4990-8300-3751ea6c741c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(auxiliary_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "7tW6xy-rQiaJ"
   },
   "outputs": [],
   "source": [
    "# Get the labels which will be split later\n",
    "y = auxiliary_features.loc[:, 'label_category'].astype(int).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "-RVt_mzpQiaL"
   },
   "outputs": [],
   "source": [
    "# Make a mask for auxiliary dataset to get all features except the one below\n",
    "column_mask_aux = ~auxiliary_features.columns.isin(['id', 'citations', 'label_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "falOi8QSQiaQ"
   },
   "outputs": [],
   "source": [
    "# # Get the columns of those auxiliary features and covert them into a list\n",
    "auxiliary = auxiliary_features.loc[:, column_mask_aux].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "r-zZMDtiQiaY",
    "outputId": "9d8855d8-31c4-4a55-8594-bbe689ebab5b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 630000/630000 [00:40<00:00, 15500.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# # Convert them into numpy array (for Keras) and stack them (if needed) as suited for the model's format\n",
    "auxiliary = [np.array(auxiliary[i][0][0] + auxiliary[i][1:]) for i in tqdm(range(len(auxiliary)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "pe_qf1g_Qiab"
   },
   "outputs": [],
   "source": [
    "# # Make a mask for time sequences features dataset to get all features except the one below\n",
    "cols = [col for col in time_sequence_features.columns if col not in ['id', 'citations', 'label_category', 'neighboring_words']]\n",
    "stripped_tsf = time_sequence_features[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "fmKEzYRYQiad"
   },
   "outputs": [],
   "source": [
    "time = stripped_tsf.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "Ly_Bd6OgQiae"
   },
   "outputs": [],
   "source": [
    "def make_structure_time_features(time_features):\n",
    "    \"\"\"\n",
    "    Concatenate features which are numbers and lists together by checking the type:\n",
    "    \n",
    "    param: time_features: the features which are considered time sequence.\n",
    "    \"\"\"\n",
    "    feature_one = np.array([int(i) for i in time_features if isinstance(i, int)])\n",
    "    feature_two = np.array([i for i in time_features if isinstance(i, list)][0])\n",
    "    return np.array([feature_one, feature_two])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "uqcj49_bQiag",
    "outputId": "4759bf9c-a156-4355-c256-d14ba821b3ea"
   },
   "outputs": [],
   "source": [
    "time = [make_structure_time_features(time[i]) for i in tqdm(range(len(time)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "nGRqP3HSQiap"
   },
   "outputs": [],
   "source": [
    "# Instantiating PCA to 35 components since it should be equal to the size of the vector of the tags\n",
    "pca = PCA(n_components=35)\n",
    "\n",
    "def get_reduced_words_dimension(data):\n",
    "    \"\"\"\n",
    "    Get the aggregated dataset of words and tags which has the\n",
    "    same dimensionality using PCA.\n",
    "    \n",
    "    :param: data: data which needs to be aggregated.\n",
    "    \"\"\"\n",
    "    tags = [i for i, _ in data]\n",
    "    word_embeddings = [j for _,j in data]\n",
    "    pca.fit(word_embeddings)\n",
    "    \n",
    "    word_embeddings_pca = pca.transform(word_embeddings)\n",
    "    tags = np.array(tags)\n",
    "    return word_embeddings_pca, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "XKPrhiqcQiau"
   },
   "outputs": [],
   "source": [
    "# Apply PCA on all the sets of data to have the dimensions of the data to be the same\n",
    "word_embeddings_pca, tags = get_reduced_words_dimension(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "O0AgxrAXQia6"
   },
   "outputs": [],
   "source": [
    "time_pca = np.dstack((word_embeddings_pca, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "A3vra1fYQia-",
    "outputId": "94c06bb1-28cc-4230-f5c1-0996c2da69a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((630000, 35), (630000, 35), (630000, 35, 2))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings_pca.shape, tags.shape, time_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "0tbprGLGQibB"
   },
   "outputs": [],
   "source": [
    "del time_sequence_features\n",
    "del auxiliary_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "4VC7y_72QibG",
    "outputId": "5cfefe3a-826b-408f-efe0-640e7238ce53"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/630000 [08:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "270"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del data\n",
    "del word_embeddings_pca\n",
    "del tags\n",
    "del stripped_tsf\n",
    "del column_mask_aux\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1lrVZ-MQibR"
   },
   "source": [
    "## LSTM/Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "id": "c3OvZ3PcQibY"
   },
   "outputs": [],
   "source": [
    "def generator_nn(features_aux, features_time, labels, batch_size):\n",
    "    \"\"\"\n",
    "    Generator to create batches of data so that processing is easy.\n",
    "\n",
    "    :param: features: the features of the model.\n",
    "    :param: labels: the labels of the model.\n",
    "    :param: batch_size: the size of the batch\n",
    "    \"\"\"\n",
    "    # Create empty arrays to contain batch of features and labels\n",
    "    # batch_features_aux = np.zeros((batch_size, 303))\n",
    "    batch_features_time =  np.zeros((batch_size, 35, 2))\n",
    "    batch_labels = np.zeros((batch_size, 3))\n",
    "  \n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            # choose random index in features\n",
    "            index = np.random.choice(len(features_aux), 1)[0]\n",
    "            # batch_features_aux[i] = features_aux[index]\n",
    "            batch_features_time[i] = features_time[index]\n",
    "            batch_labels[i] = labels[index]\n",
    "        # yield [batch_features_time, np.asarray(batch_features_aux)], batch_labels\n",
    "        yield [batch_features_time], batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "id": "yq_V7kufQibZ"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "id": "gQSpEY73Qibe"
   },
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    import math\n",
    "    if epoch <= 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * math.exp(-0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "id": "1re4aO3LQibh"
   },
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "id": "3wtMKhLBQibj"
   },
   "outputs": [],
   "source": [
    "def classification_model():\n",
    "    \"\"\"\n",
    "    Model for classifying whether a citation is scientific or not.\n",
    "    \"\"\"\n",
    "    main_input = Input(shape=(35, 2), name='time_input')\n",
    "    lstm_out = LSTM(128)(main_input)\n",
    "\n",
    "    ## only using words and tags for nwo\n",
    "    # auxiliary_input = Input(shape=(303,), name='aux_input')\n",
    "    # Converging the auxiliary input with the LSTM output\n",
    "    # x = keras.layers.concatenate([lstm_out, auxiliary_input])\n",
    "\n",
    "    # 6 fully connected layer\n",
    "    x = Dense(512, activation='relu')(lstm_out)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(64, activation='sigmoid')(x)\n",
    "\n",
    "    main_output = Dense(3, activation='softmax', name='main_output')(x)\n",
    "    # model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output])\n",
    "    model = Model(inputs=[main_input], outputs=[main_output])\n",
    "    \n",
    "    opt = Adam(lr=0.001) # SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(\n",
    "        optimizer=opt, loss={'main_output': 'categorical_crossentropy'},\n",
    "        loss_weights={'main_output': 1.}, metrics=['acc']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "id": "-djnukSUQiby",
    "outputId": "8adf1f71-0291-4d9d-cec5-f50df18bd7c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_33\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "time_input (InputLayer)         [(None, 35, 2)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_16 (LSTM)                  (None, 128)          67072       time_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "aux_input (InputLayer)          [(None, 303)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 431)          0           lstm_16[0][0]                    \n",
      "                                                                 aux_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_74 (Dense)                (None, 512)          221184      concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_75 (Dense)                (None, 256)          131328      dense_74[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_76 (Dense)                (None, 128)          32896       dense_75[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_77 (Dense)                (None, 128)          16512       dense_76[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_78 (Dense)                (None, 64)           8256        dense_77[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_79 (Dense)                (None, 32)           2080        dense_78[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 3)            99          dense_79[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 479,427\n",
      "Trainable params: 479,427\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiating the classification model\n",
    "model = classification_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_WD6dntQibz"
   },
   "source": [
    "We use `ReduceLRonPlateau` so that the model does not overshoot the optimal minimum point and hence by default we start with a learning rate of 0.01 but as soon as the accuracy stop increasing the learning rate does not change which helps us converge better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the data vars\n",
    "# from numpy import savez_compressed\n",
    "# savez_compressed('time_pca.npz', time_pca)\n",
    "# savez_compressed('auxiliary.npz', auxiliary)\n",
    "# savez_compressed('y_label.npz', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "id": "56wJeh8XQicB"
   },
   "outputs": [],
   "source": [
    "## Convert auxiliary into numpy array for indexing\n",
    "auxiliary = np.asarray(auxiliary)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "id": "LIjhKZmOQicQ"
   },
   "outputs": [],
   "source": [
    "x_train_indices, x_test_indices, y_train_indices, y_test_indices = train_test_split(\n",
    "    range(auxiliary.shape[0]), range(y.shape[0]), train_size=0.9, stratify=y, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "id": "F_jbBM-cQicj"
   },
   "outputs": [],
   "source": [
    "aux_train = auxiliary[x_train_indices]\n",
    "time_train = time_pca[x_train_indices]\n",
    "y_train = np.eye(3)[y[x_train_indices]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "id": "CXi3uFP6Qicw"
   },
   "outputs": [],
   "source": [
    "aux_test = auxiliary[x_test_indices]\n",
    "time_test = time_pca[x_test_indices]\n",
    "y_test = y[x_test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1107"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train_indices) // 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "id": "mXaf_A3pQic0",
    "outputId": "92b2a201-2b83-4f10-b219-a74b5c2a5f6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model with epochs: 50\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 512\n",
    "print('Running model with epochs: {}'.format(EPOCHS))\n",
    "\n",
    "model = None\n",
    "model = classification_model()\n",
    "training_generator = generator_nn(aux_train, time_train, y_train, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1107/1107 [==============================] - 366s 330ms/step - loss: 3.6349 - acc: 0.3860\n",
      "Epoch 2/50\n",
      "1107/1107 [==============================] - 333s 301ms/step - loss: 1.0401 - acc: 0.4898\n",
      "Epoch 3/50\n",
      "1107/1107 [==============================] - 327s 296ms/step - loss: 1.2532 - acc: 0.4832\n",
      "Epoch 4/50\n",
      "1107/1107 [==============================] - 315s 285ms/step - loss: 0.9545 - acc: 0.5302\n",
      "Epoch 5/50\n",
      "1107/1107 [==============================] - 318s 287ms/step - loss: 0.9614 - acc: 0.5376\n",
      "Epoch 6/50\n",
      "1107/1107 [==============================] - 319s 288ms/step - loss: 0.9074 - acc: 0.5582\n",
      "Epoch 7/50\n",
      "1107/1107 [==============================] - 318s 287ms/step - loss: 0.8989 - acc: 0.5639\n",
      "Epoch 8/50\n",
      "1107/1107 [==============================] - 318s 287ms/step - loss: 0.8918 - acc: 0.5682\n",
      "Epoch 9/50\n",
      "1107/1107 [==============================] - 321s 290ms/step - loss: 0.8988 - acc: 0.5706\n",
      "Epoch 10/50\n",
      "1107/1107 [==============================] - 316s 286ms/step - loss: 0.8647 - acc: 0.5847\n",
      "Epoch 11/50\n",
      "1107/1107 [==============================] - 313s 282ms/step - loss: 0.8577 - acc: 0.5898\n",
      "Epoch 12/50\n",
      "1107/1107 [==============================] - 312s 282ms/step - loss: 0.8546 - acc: 0.5911\n",
      "Epoch 13/50\n",
      "1107/1107 [==============================] - 313s 283ms/step - loss: 0.8433 - acc: 0.5987\n",
      "Epoch 14/50\n",
      "1107/1107 [==============================] - 315s 284ms/step - loss: 0.8330 - acc: 0.6047\n",
      "Epoch 15/50\n",
      "1107/1107 [==============================] - 313s 283ms/step - loss: 0.8242 - acc: 0.6099\n",
      "Epoch 16/50\n",
      "1107/1107 [==============================] - 318s 287ms/step - loss: 0.8232 - acc: 0.6108\n",
      "Epoch 17/50\n",
      "1107/1107 [==============================] - 311s 281ms/step - loss: 0.8139 - acc: 0.6164\n",
      "Epoch 18/50\n",
      "1107/1107 [==============================] - 308s 279ms/step - loss: 0.8051 - acc: 0.6213\n",
      "Epoch 19/50\n",
      "1107/1107 [==============================] - 309s 279ms/step - loss: 0.7912 - acc: 0.6292\n",
      "Epoch 20/50\n",
      "1107/1107 [==============================] - 313s 283ms/step - loss: 0.7894 - acc: 0.6304\n",
      "Epoch 21/50\n",
      "1107/1107 [==============================] - 316s 285ms/step - loss: 0.7821 - acc: 0.6342\n",
      "Epoch 22/50\n",
      "1107/1107 [==============================] - 314s 284ms/step - loss: 0.7743 - acc: 0.6381\n",
      "Epoch 23/50\n",
      "1107/1107 [==============================] - 314s 284ms/step - loss: 0.7723 - acc: 0.6400\n",
      "Epoch 24/50\n",
      "1107/1107 [==============================] - 314s 283ms/step - loss: 0.7653 - acc: 0.6437\n",
      "Epoch 25/50\n",
      "1107/1107 [==============================] - 313s 283ms/step - loss: 0.7621 - acc: 0.6454\n",
      "Epoch 26/50\n",
      "1107/1107 [==============================] - 316s 286ms/step - loss: 0.7571 - acc: 0.6479\n",
      "Epoch 27/50\n",
      "1107/1107 [==============================] - 315s 284ms/step - loss: 0.7528 - acc: 0.6503\n",
      "Epoch 28/50\n",
      "1107/1107 [==============================] - 315s 284ms/step - loss: 0.7499 - acc: 0.6517\n",
      "Epoch 29/50\n",
      "1107/1107 [==============================] - 314s 284ms/step - loss: 0.7462 - acc: 0.6536\n",
      "Epoch 30/50\n",
      "1107/1107 [==============================] - 314s 284ms/step - loss: 0.7434 - acc: 0.6553\n",
      "Epoch 31/50\n",
      "1107/1107 [==============================] - 322s 291ms/step - loss: 0.7414 - acc: 0.6561\n",
      "Epoch 32/50\n",
      "1107/1107 [==============================] - 315s 284ms/step - loss: 0.7370 - acc: 0.6584\n",
      "Epoch 33/50\n",
      "1107/1107 [==============================] - 312s 282ms/step - loss: 0.7354 - acc: 0.6585\n",
      "Epoch 34/50\n",
      "1107/1107 [==============================] - 313s 283ms/step - loss: 0.7346 - acc: 0.6596\n",
      "Epoch 35/50\n",
      "1107/1107 [==============================] - 314s 284ms/step - loss: 0.7329 - acc: 0.6588\n",
      "Epoch 36/50\n",
      "1107/1107 [==============================] - 314s 284ms/step - loss: 0.7302 - acc: 0.6621\n",
      "Epoch 37/50\n",
      "1107/1107 [==============================] - 314s 284ms/step - loss: 0.7311 - acc: 0.6602\n",
      "Epoch 38/50\n",
      "1107/1107 [==============================] - 314s 284ms/step - loss: 0.7272 - acc: 0.6632\n",
      "Epoch 39/50\n",
      "1107/1107 [==============================] - 314s 284ms/step - loss: 0.7259 - acc: 0.6638\n",
      "Epoch 40/50\n",
      "1107/1107 [==============================] - 313s 283ms/step - loss: 0.7242 - acc: 0.6642\n",
      "Epoch 41/50\n",
      "1107/1107 [==============================] - 314s 284ms/step - loss: 0.7237 - acc: 0.6651\n",
      "Epoch 42/50\n",
      "1107/1107 [==============================] - 314s 284ms/step - loss: 0.7202 - acc: 0.6668\n",
      "Epoch 43/50\n",
      "1107/1107 [==============================] - 314s 284ms/step - loss: 0.7223 - acc: 0.6659\n",
      "Epoch 44/50\n",
      "1107/1107 [==============================] - 316s 286ms/step - loss: 0.7214 - acc: 0.6659\n",
      "Epoch 45/50\n",
      "1107/1107 [==============================] - 314s 284ms/step - loss: 0.7188 - acc: 0.6668\n",
      "Epoch 46/50\n",
      "1107/1107 [==============================] - 314s 284ms/step - loss: 0.7197 - acc: 0.6673\n",
      "Epoch 47/50\n",
      "1107/1107 [==============================] - 314s 284ms/step - loss: 0.7184 - acc: 0.6673\n",
      "Epoch 48/50\n",
      "1107/1107 [==============================] - 313s 283ms/step - loss: 0.7174 - acc: 0.6680\n",
      "Epoch 49/50\n",
      "1107/1107 [==============================] - 313s 283ms/step - loss: 0.7170 - acc: 0.6686\n",
      "Epoch 50/50\n",
      "1107/1107 [==============================] - 314s 283ms/step - loss: 0.7174 - acc: 0.6681\n"
     ]
    }
   ],
   "source": [
    "history_callback = model.fit_generator(\n",
    "    training_generator,\n",
    "    steps_per_epoch=len(x_train_indices) // BATCH_SIZE,\n",
    "    epochs=EPOCHS, verbose=1, shuffle=True, callbacks=[callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "id": "jjxzVyzqQidC"
   },
   "outputs": [],
   "source": [
    "history_dict = history_callback.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "id": "pghfYn-FQidE"
   },
   "outputs": [],
   "source": [
    "f = open('./citation_model_loss_{}.json'.format(EPOCHS), 'w')\n",
    "f.write(str(history_dict))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./citation_model_epochs_{}.h5'.format(EPOCHS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "id": "z6_3iHafQidI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Neural network model for epochs 50: 0.6293968253968254\n",
      "       CONS   LIBR   MODR  accuracy\n",
      "CONS  15017   4029   1954  0.629397\n",
      "LIBR   5036  13231   2733  0.629397\n",
      "MODR   4345   5251  11404  0.629397\n"
     ]
    }
   ],
   "source": [
    "prediction_for_folds = model.predict([time_test, aux_test])\n",
    "y_pred = np.argmax(prediction_for_folds, axis=1)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of the Neural network model for epochs {}: {}\".format(EPOCHS, accuracy))\n",
    "\n",
    "res = pd.DataFrame(confusion_matrix(y_test, y_pred))\n",
    "res.index = ['CONS', 'LIBR', 'MODR']\n",
    "res.columns = ['CONS', 'LIBR', 'MODR']\n",
    "res['accuracy'] = accuracy\n",
    "res.to_csv('./citation_model_result_{}.csv'.format(EPOCHS))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-Cq1A-HQidL"
   },
   "outputs": [],
   "source": [
    "model.save('./citation_model_epochs_{}.h5'.format(EPOCHS))\n",
    "json_string = model.to_json()\n",
    "with open(\"./citation_model_epochs_{}.json\".format(EPOCHS), \"w\") as json_file:\n",
    "    json_file.write(json_string)\n",
    "\n",
    "print('\\n\\nDone with the prediction and saving model with epochs: {}\\n'.format(EPOCHS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dOM2ClWiQidQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUYVLRLkQidS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yz20FlVhQidT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "F9qpl7i9QiQv",
    "bdH9m5NsQiTX",
    "Zi5d0kjlQiZt",
    "EEV9WDWaQiaG",
    "l1lrVZ-MQibR"
   ],
   "name": "check_bias_NN_3_labels.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
